<!--
## -------------------------------------------------------------------------------------
## Sistemas Inteligentes para la Gestión en la Empresa
## Curso 2018-2019
## Carlos Cobos Suárez, Adrián Morente Gabaldón
## Práctica 2: Deep Learning para multi-clasificación
## -------------------------------------------------------------------------------------
-->

Definición de las carpetas de trabajo
```{r echo=FALSE}
remove(list = ls())
setwd("~/Desktop/practica2") # o donde sea
images_origin  <- './train_images'
train_dir      <- './train/'
# validation_dir <- './validation/' 
test_dir       <- './test/'
```

Función para crear subconjuntos con las imágenes
```{r}
separaDatos <- function(carpeta_entrada, carpeta_salida, porcentaje = 0.2, borrar_entrada=TRUE) {
  if (!dir.exists(carpeta_salida))
    dir.create(carpeta_salida)
  
  clases <- list.dirs(path = carpeta_entrada, full.names = FALSE)
  
  for (clase in clases){
    if(clase != "") {
      carpeta_clase_entrada <- paste(carpeta_entrada,clase,sep = "/")
      carpeta_clase_salida <- paste(carpeta_salida,clase,sep = "/")
      if (!dir.exists(carpeta_clase_salida))
        dir.create(carpeta_clase_salida)
      
      todos <- list.files(path = carpeta_clase_entrada)
      a_copiar <- sample(todos, length(todos)*porcentaje)
      
      for (fichero in a_copiar){
        file.copy(paste(carpeta_clase_entrada, fichero, sep = "/"), carpeta_clase_salida)
        if (borrar_entrada)
          file.remove(paste(carpeta_clase_entrada, fichero, sep = "/"))
      }
    }
  }
}

```

Función para contabilizar las imágenes de cada clase
```{r}
cuentaDatos <- function(carpeta_entrada) {
  clases <- list.dirs(path = carpeta_entrada, full.names = FALSE)
  num <- rep(0, length(clases)-1)
  cl = 1
  for (clase in clases){
    if(clase != "") {
      carpeta_clase_entrada <- paste(carpeta_entrada,clase,sep = "/")
      
      todos <- list.files(path = carpeta_clase_entrada)
      num[cl] <- length(todos)
      cl <- cl + 1
    }
  }
  data.frame(num, "clase"=c("0","1","2","3","4"))
}
```

Análisis previo de los datos
```{r}
library(ggplot2)
conteo_dataset_original <- cuentaDatos(images_origin)
bp_dataset_original <- ggplot(data=conteo_dataset_original, aes(x=clase, y=num)) +
  geom_bar(stat="identity", fill="steelblue")+
  geom_text(aes(label=num), vjust=-0.3, size=3.5)+
  theme_minimal()
ggsave("barplot_original.svg",bp_dataset_original, device = "svg")
bp_dataset_original
```

Como no tenemos un conjunto de prueba para verificar el funcionamiento de la red neuronal, vamos a sacar del conjunto de entrenamiento un subconjunto estratificado para test, es decir, respetando la proporción de las clases.
```{r}
set.seed(333.857)
separaDatos(images_origin, train_dir, 1, FALSE) # Se copian todos los datos de train
separaDatos(train_dir, test_dir, 0.2)           # De dicho train, se coge el 20% para test
# separaDatos(train_dir, validation_dir, 0.2)     # Del train se vuelve a coger el 20% para v

conteo_particionado <- data.frame(
  "num"=c(cuentaDatos(train_dir)$num, cuentaDatos(test_dir)$num), 
  "conjunto"=c(rep("train",5),rep("test",5)),
  "clases" = rep(c(0,1,2,3,4),2)
  )

bp_dataset_particionado <- ggplot(data=conteo_particionado, aes(x=clases, y=num, fill=conjunto)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=num), vjust=-0.3,
            position = position_dodge(0.9), size=3.5)+
  scale_fill_brewer(palette="Paired")+
  theme_minimal()
ggsave("bp_dataset_particionado.svg",bp_dataset_particionado, device = "svg")
bp_dataset_particionado

```

Como se puede apreciar, hay un desbalanceo muy considerable de los datos, sobretodo en la clase 0 donde a penas hay.
Una vez se ha analizado superficialmente los datos del dataset, cabe indicar que sólamente con las imágenes no se puede clasificar correctamente a los animales en las clases de adopción.

Es hora de probar diversas configuraciones de redes para intentar clasificar a los animales. Para que no demore mucho, optamos por utilizar solo el 50% de losd datos. Para poder comparar los diferentes modelos, se van a usar la función de pérdida *categorical_crossentropy* y la de precisión *categorical_accuracy*. Una vez que se sepa qué configuración de red va mejor, se le pasará el conjunto de datos entero.

```{r}
train_50_dir      <- './train50/'
test_50_dir       <- './test50/'

separaDatos(train_dir, train_50_dir, 0.5, FALSE)
separaDatos(test_dir, test_50_dir, 0.5, F)
library(keras)
```

Primer modelo: Ejemplo dogs_cats visto en clase pero modificado para clasificación categórica:
```{r}
# Leer de datos
train_datagen      <- image_data_generator(rescale = 1/255, validation_split = 0.2)
test_datagen       <- image_data_generator(rescale = 1/255)

train_data <- flow_images_from_directory(
  directory = train_50_dir,
  generator = train_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  subset='training',
  seed = 333.857
)
validation_data <- flow_images_from_directory(
  directory = train_50_dir,
  generator = train_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  subset='validation',
  seed = 333.857
)
test_data <- flow_images_from_directory(
  directory = test_50_dir,
  generator = test_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  seed = 333.857
)

# Definir arquitectura
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  # Red tradicional
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  
  # Capa de salida
  layer_dense(units = 5, activation = "sigmoid") # 5 unidades de salida, una por categoría

summary(model)

# Compilar modelo
model %>% compile(
  loss = "categorical_crossentropy", # Tiene que ser categórico, tenemos 5 clases
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("categorical_accuracy")
)

# Entrenamiento
history <- model %>% 
  fit_generator(
    train_data,
    steps_per_epoch = 50,
    epochs = 15,
    validation_data = validation_data,
    validation_steps = 100
  )

# Evaluar modelo
puntiacion1 <- model %>% evaluate_generator(test_data, steps = 220)
# $loss
# [1] 1.449931
# 
# $categorical_accuracy
# [1] 0.2842959

# Visualizar entrenamiento
plot(history)
```

Segundo modelo: Balanceo de clases:
```{r}
# Leer de datos
train_datagen      <- image_data_generator(rescale = 1/255, validation_split = 0.2)
test_datagen       <- image_data_generator(rescale = 1/255)

train_data <- flow_images_from_directory(
  directory = train_50_dir,
  generator = train_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  subset='training',
  seed = 333.857
)
validation_data <- flow_images_from_directory(
  directory = train_50_dir,
  generator = train_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  subset='validation',
  seed = 333.857
)
test_data <- flow_images_from_directory(
  directory = test_50_dir,
  generator = test_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  seed = 333.857
)

# Definir arquitectura
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  # Red tradicional
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  
  # Capa de salida
  layer_dense(units = 5, activation = "sigmoid") # 5 unidades de salida, una por categoría

summary(model)

# Compilar modelo
model %>% compile(
  loss = "categorical_crossentropy", # Tiene que ser categórico, tenemos 5 clases
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("categorical_accuracy")
)

# Entrenamiento
history <- model %>% 
  fit_generator(
    train_data,
    steps_per_epoch = 50,
    epochs = 15,
    validation_data = validation_data,
    validation_steps = 100,
    class_weight = list("0"=10,"1"=1.1,"2"=0.84,"3"=1,"4"=1)
  )

# Evaluar modelo
puntuacion2 <- model %>% evaluate_generator(test_data, steps = 220)

# $loss
# [1] 1.665951
# 
# $categorical_accuracy
# [1] 0.2321551

# Visualizar entrenamiento
plot(history)
```

Este modelo empeora. Puede ser que sin balancear, el modelo acierte más ya que lleva los datos a las clases mayoritarias.
Habría que ver la asignación de etiquetas de los modelos.

Tercer modelo: Data augmentation:
```{r}
# Leer de datos
train_datagen      <- image_data_generator(rescale = 1/255, validation_split = 0.2, horizontal_flip = TRUE, vertical_flip = TRUE, rotation_range = 30) 
test_datagen       <- image_data_generator(rescale = 1/255)

train_data <- flow_images_from_directory(
  directory = train_50_dir,
  generator = train_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  subset='training',
  seed = 333.857
)
validation_data <- flow_images_from_directory(
  directory = train_50_dir,
  generator = train_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  subset='validation',
  seed = 333.857
)
test_data <- flow_images_from_directory(
  directory = test_50_dir,
  generator = test_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  seed = 333.857
)

# Definir arquitectura
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  # Red tradicional
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  
  # Capa de salida
  layer_dense(units = 5, activation = "sigmoid") # 5 unidades de salida, una por categoría

summary(model)

# Compilar modelo
model %>% compile(
  loss = "categorical_crossentropy", # Tiene que ser categórico, tenemos 5 clases
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("categorical_accuracy")
)

# Entrenamiento
history <- model %>% 
  fit_generator(
    train_data,
    steps_per_epoch = 100,
    epochs = 15,
    validation_data = validation_data,
    validation_steps = 100
  )

# Evaluar modelo
puntuacion3 <- model %>% evaluate_generator(test_data, steps = 220)

# $loss
# [1] 1.454917
# 
# $categorical_accuracy
# [1] 0.2892931

# Visualizar entrenamiento
plot(history)
```


Técnicas de binarización: OVO

```{r}
# Definir grupos de OVO
gruposOVO <- list(c("0","1"),c("0","2"),c("0","3"),c("0","4"),c("1","2"),c("1","3"),c("1","4"),c("2","3"),c("2","4"),c("3","4"))

train_datagen      <- image_data_generator(rescale = 1/255, validation_split = 0.2) 
test_datagen       <- image_data_generator(rescale = 1/255)

puntuacionOVO = data.frame("NA"=c(1,1), row.names = c("loss","acc"))

for (dosClases in gruposOVO) {
  train_data <- flow_images_from_directory(
    directory = train_50_dir,
    generator = train_datagen,
    target_size = c(150, 150),
    batch_size = 50,
    class_mode = "binary",
    subset='training',
    seed = 333.857,
    classes = unlist(dosClases)
  )
  validation_data <- flow_images_from_directory(
    directory = train_50_dir,
    generator = train_datagen,
    target_size = c(150, 150),
    batch_size = 50,
    class_mode = "binary",
    subset='validation',
    seed = 333.857,
    classes = unlist(dosClases)
  )
  test_data <- flow_images_from_directory(
    directory = test_50_dir,
    generator = test_datagen,
    target_size = c(150, 150),
    batch_size = 50,
    class_mode = "binary",
    seed = 333.857,
    classes = unlist(dosClases)
  )
  
  # Definir arquitectura
  model <- keras_model_sequential() %>%
    layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = "relu", input_shape = c(150, 150, 3)) %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = "relu") %>% 
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    
    # Red tradicional
    layer_flatten() %>%
    layer_dense(units = 512, activation = "relu") %>%
    
    # Capa de salida
    layer_dense(units = 1, activation = "sigmoid") # 5 unidades de salida, una por categoría
  
  summary(model)
  
  # Compilar modelo
  model %>% compile(
    loss = "binary_crossentropy",
    optimizer = optimizer_rmsprop(lr = 1e-4),
    metrics = c("accuracy")
  )
  
  # Entrenamiento
  history <- model %>% 
    fit_generator(
      train_data,
      steps_per_epoch = 30,
      epochs = 15,
      validation_data = validation_data,
      validation_steps = 100
    )
  
  # Evaluar modelo
  puntuacionDC <- model %>% evaluate_generator(test_data, steps = 100)
  
  model %>% save_model_hdf5(paste("petfinder_ovo_",paste(unlist(dosClases), sep = "", collapse = ""),".h5", sep = ""))
  
  png(paste("ovo_",paste(unlist(dosClases), sep = "", collapse = ""),".png", sep = ""))
  plot(history)
  dev.off()
  
  ptn <- data.frame("pun"=unlist(puntuacionDC))
  names(ptn) <- paste(unlist(dosClases), sep = "", collapse = "")
  puntuacionOVO <- cbind(puntuacionOVO, ptn)
}

puntuacionOVO <- puntuacionOVO[,-1]
puntuacionOVO
```

Como se puede ver, la clase 0 con las demás da muy buen resultado debido al desbalanceo tan grande. Las demás prácticamente son clasificadores aleatorios.

Vamos a hacer lo mismo pero balanceando las clases:

```{r}

puntuacionOVO_bal <- data.frame("NA"=c(1,1), row.names = c("loss","acc"))

for (dosClases in gruposOVO) {
# dosClases <- gruposOVO[2]
  print(dosClases)
  
  train_data <- flow_images_from_directory(
    directory = train_50_dir,
    generator = train_datagen,
    target_size = c(150, 150),
    batch_size = 50,
    class_mode = "binary",
    subset='training',
    seed = 333.857,
    classes = unlist(dosClases)
  )
  validation_data <- flow_images_from_directory(
    directory = train_50_dir,
    generator = train_datagen,
    target_size = c(150, 150),
    batch_size = 50,
    class_mode = "binary",
    subset='validation',
    seed = 333.857,
    classes = unlist(dosClases)
  )
  test_data <- flow_images_from_directory(
    directory = test_50_dir,
    generator = test_datagen,
    target_size = c(150, 150),
    batch_size = 50,
    class_mode = "binary",
    seed = 333.857,
    classes = unlist(dosClases)
  )
  
  # Definir arquitectura
  model <- keras_model_sequential() %>%
    layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = "relu", input_shape = c(150, 150, 3)) %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = "relu") %>% 
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    
    # Red tradicional
    layer_flatten() %>%
    layer_dense(units = 512, activation = "relu") %>%
    
    # Capa de salida
    layer_dense(units = 1, activation = "sigmoid") # 5 unidades de salida, una por categoría
  
  summary(model)
  
  # Compilar modelo
  model %>% compile(
    loss = "binary_crossentropy",
    optimizer = optimizer_rmsprop(lr = 1e-4),
    metrics = c("accuracy")
  )
  
  # Balancear
  datos1 <- length(train_data$labels[train_data$labels==0])
  datos2 <- length(train_data$labels[train_data$labels==1])
  
  pesos <- list(datos2/datos1, 1) # Como la clase minoritaria es la primera, se hace oversampling
  names(pesos) <- c(0, 1)
  print(pesos)
  
  # Entrenamiento
  history <- model %>% 
    fit_generator(
      train_data,
      steps_per_epoch = 30,
      epochs = 15,
      validation_data = validation_data,
      validation_steps = 100,
      class_weight = pesos # Balanceo de clases
    )
  
  # Evaluar modelo
  puntuacionDC <- model %>% evaluate_generator(test_data, steps = 100)
  
  model %>% save_model_hdf5(paste("petfinder_ovo_bal_",paste(unlist(dosClases), sep = "", collapse = ""),".h5", sep = ""))
  
  png(paste("ovo_bal_",paste(unlist(dosClases), sep = "", collapse = ""),".png", sep = ""))
  plot(history)
  dev.off()
  
  plot(history)
  
  ptn <- data.frame("pun"=unlist(puntuacionDC))
  names(ptn) <- paste(unlist(dosClases), sep = "", collapse = "")
  puntuacionOVO_bal <- cbind(puntuacionOVO_bal, ptn)
}

puntuacionOVO_bal <- puntuacionOVO_bal[,-1]
puntuacionOVO_bal
```

Prueba con el código de dog_cats del profesor. Algunas cosas importantes se han cambiado y debidamente comentado.
Lo suyo sería coger un porcentaje de los datos para comparar los distintos modelos y así quue no tarde mucho.

Modelos:
 * Sin nada = 0.2873636
 * Sin balanceo, con 1 = 0.2819
 * Sin balanceo, con 1 y 2 = 0.2819
 * Con data augmentation = 0.2831818
 * Con data augmentation y balanceo = 0.2582727
 * Red para probar http://cbonnett.github.io/Insight.html

Una vez que sepamos qué tipo de red va mejor, a esa se le pasan todos los datos

```{r}
# https://tensorflow.rstudio.com/keras/reference/image_data_generator.html 
train_datagen      <- image_data_generator(rescale = 1/255) #, horizontal_flip = TRUE, vertical_flip = TRUE, rotation_range = 30) 
validation_datagen <- image_data_generator(rescale = 1/255)
test_datagen       <- image_data_generator(rescale = 1/255)

# https://tensorflow.rstudio.com/keras/reference/flow_images_from_directory.html
train_data <- flow_images_from_directory(
  directory = train_dir,
  generator = train_datagen,
  target_size = c(150, 150),   # (w, h) --> (150, 150)
  batch_size = 50,             # grupos de 50 imágenes
  class_mode = "categorical",   # tenemos 5 categorías #"binary"        # etiquetas binarias
  seed = 333.857
)

validation_data <- flow_images_from_directory(
  directory = validation_dir,
  generator = validation_datagen,
  target_size = c(150, 150),   # (w, h) --> (150, 150)
  batch_size = 50,             # grupos de 50 imágenes
  class_mode = "categorical",   # tenemos 5 categorías
  seed = 333.857
)

test_data <- flow_images_from_directory(
  directory = test_dir,
  generator = test_datagen,
  target_size = c(150, 150),   # (w, h) --> (150, 150)
  batch_size = 50,             # grupos de 50 imágenes
  class_mode = "categorical",   # tenemos 5 categorías
  seed = 333.857
)

## -------------------------------------------------------------------------------------
## Crear modelo

# Definir arquitectura
# https://tensorflow.rstudio.com/keras/articles/sequential_model.html
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  # Red tradicional
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  # # 1
  # layer_dropout(rate = 0.15) %>% 
  # layer_dense(units = 256, activation = "relu") %>%
  # # 2
  # layer_dropout(rate = 0.15) %>% 
  # layer_dense(units = 128, activation = "relu") %>%
  
  # Capa de salida
  layer_dense(units = 5, activation = "sigmoid") # 5 unidades de salida, una por categoría

summary(model)

# Compilar modelo
# https://tensorflow.rstudio.com/keras/reference/compile.html
model %>% compile(
  loss = "categorical_crossentropy", # Tiene que ser categórico, tenemos 5 clases
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("categorical_accuracy")
)

# Entrenamiento
# https://tensorflow.rstudio.com/keras/reference/fit_generator.html
history <- model %>% 
  fit_generator(
    train_data,
    steps_per_epoch = 50,
    epochs = 15,
    validation_data = validation_data,
    validation_steps = 100,
    #class_weight = list("0"=10,"1"=1.1,"2"=0.84,"3"=1,"4"=1) # Balanceo de clases
  )

# Evaluar modelo
# https://tensorflow.rstudio.com/keras/reference/evaluate_generator.html
model %>% evaluate_generator(test_data, steps = 220)

# Sin nada = 0.2873636
# Sin balanceo, con 1 = 0.2819
# Sin balanceo, con 1 y 2 = 0.2819
# Con data augmentation = 0.2831818
# Con data augmentation y balanceo = 0.2582727


# Guardar modelo (HDF5)
# https://tensorflow.rstudio.com/keras/reference/save_model_hdf5.html
# model %>% save_model_hdf5("petfinder_v1.h5")

# Visualizar entrenamiento
plot(history)
```


Red para el CSV
https://www.tensorflow.org/alpha/tutorials/keras/feature_columns
Hay que hacer una red aparte
https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/
```{r}
library(readr)
train <- read_csv("train.csv")

library(tibble)
library(dplyr)

column_names <- colnames(train)
train_df <- as_tibble(train)
colnames(train_df) <- column_names

y = train_df %>% select(PetID, AdoptionSpeed)
train_df = select(train_df,-c(AdoptionSpeed, PhotoAmt))


```

Modelo cogido de la web de cbonnett
```{r}
# Leer de datos
train_datagen      <- image_data_generator(rescale = 1/255, validation_split = 0.2)
test_datagen       <- image_data_generator(rescale = 1/255)

train_data <- flow_images_from_directory(
  directory = train_50_dir,
  generator = train_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  subset='training',
  seed = 333.857
)
validation_data <- flow_images_from_directory(
  directory = train_50_dir,
  generator = train_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  subset='validation',
  seed = 333.857
)
test_data <- flow_images_from_directory(
  directory = test_50_dir,
  generator = test_datagen,
  target_size = c(150, 150),
  batch_size = 50,
  class_mode = "categorical",
  seed = 333.857
)

# Definir arquitectura
model <- keras_model_sequential() %>%
  layer_zero_padding_2d(padding = c(1L, 1L), data_format = c(3,150, 150)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2)) %>%
  # 
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2)) %>%
  # 
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 256, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 256, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 256, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2)) %>%
  # 
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 512, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 512, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 512, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2)) %>%
  # 
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 512, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 512, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_zero_padding_2d(padding = c(1L, 1L)) %>%
  # layer_conv_2d(filters = 512, kernel_size = c(3, 3), activation = "relu") %>%
  # layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2)) %>%
  
  # Red tradicional
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  
  # Capa de salida
  layer_dense(units = 5, activation = "sigmoid") # 5 unidades de salida, una por categoría

summary(model)

# Compilar modelo
model %>% compile(
  loss = "categorical_crossentropy", # Tiene que ser categórico, tenemos 5 clases
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("categorical_accuracy")
)

# Entrenamiento
history <- model %>% 
  fit_generator(
    train_data,
    steps_per_epoch = 50,
    epochs = 15,
    validation_data = validation_data,
    validation_steps = 100
  )

# Evaluar modelo
model %>% evaluate_generator(test_data, steps = 220)
# $loss
# [1] ¿?
# 
# $categorical_accuracy
# [1] ¿?

# Visualizar entrenamiento
plot(history)
```